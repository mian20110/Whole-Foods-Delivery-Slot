{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mian20110/Whole-Foods-Delivery-Slot/blob/master/BERT/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACxM9zDQ3X2G"
      },
      "source": [
        "import random\n",
        "import re\n",
        "from math import sqrt as msqrt\n",
        "\n",
        "import torch\n",
        "import torch.functional as F\n",
        "from torch import nn\n",
        "from torch.optim import Adadelta\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GE3xokJEmyC"
      },
      "source": [
        "max_len = 30\n",
        "max_vocab = 50\n",
        "max_pred = 5\n",
        "\n",
        "d_k = d_v = 64\n",
        "d_model = 768  # n_heads * d_k\n",
        "d_ff = d_model * 4\n",
        "\n",
        "n_heads = 12\n",
        "n_layers = 6\n",
        "n_segs = 2\n",
        "\n",
        "p_dropout = .1\n",
        "# BERT propability defined\n",
        "p_mask = .8\n",
        "p_replace = .1\n",
        "p_do_nothing = 1 - p_mask - p_replace\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = torch.device(device)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anpZ6ZxQG9fi"
      },
      "source": [
        "$$\n",
        "\\displaylines{\n",
        "\\operatorname{GELU}(x)=x P(X \\leq x)= x \\Phi(x)=x \\cdot \\frac{1}{2}[1+\\operatorname{erf}(x / \\sqrt{2})] \\\\\n",
        " or \\\\\n",
        "0.5 x\\left(1+\\tanh \\left[\\sqrt{2 / \\pi}\\left( x+ 0.044715 x^{3}\\right)\\right]\\right)\n",
        "}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxcx2VkW_Cws"
      },
      "source": [
        "def gelu(x):\n",
        "    '''\n",
        "    Two way to implements GELU:\n",
        "    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "    or\n",
        "    0.5 * x * (1. + torch.erf(torch.sqrt(x, 2)))\n",
        "    '''\n",
        "    return .5 * x * (1. + torch.erf(x / msqrt(2.)))\n",
        "\n",
        "\n",
        "def get_pad_mask(tokens, pad_idx=0):\n",
        "    '''\n",
        "    suppose index of [PAD] is zero in word2idx\n",
        "    tokens: [batch, seq_len]\n",
        "    '''\n",
        "    batch, seq_len = tokens.size()\n",
        "    pad_mask = tokens.data.eq(pad_idx).unsqueeze(1)\n",
        "    pad_mask = pad_mask.expand(batch, seq_len, seq_len)\n",
        "    return pad_mask"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2vrJaRFEtMM"
      },
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.seg_emb = nn.Embedding(n_segs, d_model)\n",
        "        self.word_emb = nn.Embedding(max_vocab, d_model)\n",
        "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(p_dropout)\n",
        "\n",
        "    def forward(self, x, seg):\n",
        "        '''\n",
        "        x: [batch, seq_len]\n",
        "        '''\n",
        "        word_enc = self.word_emb(x)\n",
        "\n",
        "        # positional embedding\n",
        "        pos = torch.arange(x.shape[1], dtype=torch.long, device=device)\n",
        "        pos = pos.unsqueeze(0).expand_as(x)\n",
        "        pos_enc = self.pos_emb(pos)\n",
        "\n",
        "        seg_enc = self.seg_emb(seg)\n",
        "        x = self.norm(word_enc + pos_enc + seg_enc)\n",
        "        return self.dropout(x)\n",
        "        # return: [batch, seq_len, d_model]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Avp2WLT7F4Bz"
      },
      "source": [
        "$$\n",
        "\\operatorname{Attention}(Q, K, V) = \\operatorname{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\operatorname{MultiHead}(Q, K, V) &= \\operatorname{Concat}(\\text{head}_1, \\text{head}_2, \\dots, \\text{head}_h)W^O \\\\\n",
        "\\text{where } \\text{head}_i &= \\operatorname{Attention}(QW^Q_i, KW^K_i, VW^V_i)\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grXsJR7hEwNO"
      },
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2) / msqrt(d_k))\n",
        "        # scores: [batch, n_heads, seq_len, seq_len]\n",
        "        scores.masked_fill_(attn_mask, -1e9)\n",
        "        attn = nn.Softmax(dim=-1)(scores)\n",
        "        # context: [batch, n_heads, seq_len, d_v]\n",
        "        context = torch.matmul(attn, V)\n",
        "        return context\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=False)\n",
        "        self.fc = nn.Linear(n_heads * d_v, d_model, bias=False)\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        '''\n",
        "        Q, K, V: [batch, seq_len, d_model]\n",
        "        attn_mask: [batch, seq_len, seq_len]\n",
        "        '''\n",
        "        batch = Q.size(0)\n",
        "        '''\n",
        "        split Q, K, V to per head formula: [batch, seq_len, n_heads, d_k]\n",
        "        Convenient for matrix multiply opearation later\n",
        "        q, k, v: [batch, n_heads, seq_len, d_k / d_v]\n",
        "        '''\n",
        "        per_Q = self.W_Q(Q).view(batch, -1, n_heads, d_k).transpose(1, 2)\n",
        "        per_K = self.W_K(K).view(batch, -1, n_heads, d_k).transpose(1, 2)\n",
        "        per_V = self.W_V(V).view(batch, -1, n_heads, d_v).transpose(1, 2)\n",
        "\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
        "        # context: [batch, n_heads, seq_len, d_v]\n",
        "        context = ScaledDotProductAttention()(per_Q, per_K, per_V, attn_mask)\n",
        "        context = context.transpose(1, 2).contiguous().view(\n",
        "            batch, -1, n_heads * d_v)\n",
        "\n",
        "        # output: [batch, seq_len, d_model]\n",
        "        output = self.fc(context)\n",
        "        return output"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMv1kcbsEzA_"
      },
      "source": [
        "class FeedForwardNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeedForwardNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(p_dropout)\n",
        "        self.gelu = gelu\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OahVIupjE7XV"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.enc_attn = MultiHeadAttention()\n",
        "        self.ffn = FeedForwardNetwork()\n",
        "\n",
        "    def forward(self, x, pad_mask):\n",
        "        '''\n",
        "        pre-norm\n",
        "        see more detail in https://openreview.net/pdf?id=B1x8anVFPr\n",
        "\n",
        "        x: [batch, seq_len, d_model]\n",
        "        '''\n",
        "        residual = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.enc_attn(x, x, x, pad_mask) + residual\n",
        "        residual = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ffn(x)\n",
        "        return x + residual"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-34bC9CE-Vt"
      },
      "source": [
        "class Pooler(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Pooler, self).__init__()\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        x: [batch, d_model] (first place output)\n",
        "        '''\n",
        "        x = self.fc(x)\n",
        "        x = self.tanh(x)\n",
        "        return x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASRF_NKiFBkk"
      },
      "source": [
        "class BERT(nn.Module):\n",
        "    def __init__(self, n_layers):\n",
        "        super(BERT, self).__init__()\n",
        "        self.embedding = Embeddings()\n",
        "        self.encoders = nn.ModuleList([\n",
        "            EncoderLayer() for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        self.pooler = Pooler()\n",
        "\n",
        "        self.next_cls = nn.Linear(d_model, 2)\n",
        "        self.gelu = gelu\n",
        "\n",
        "        shared_weight = self.pooler.fc.weight\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "        self.fc.weight = shared_weight\n",
        "\n",
        "        shared_weight = self.embedding.word_emb.weight\n",
        "        self.word_classifier = nn.Linear(d_model, max_vocab, bias=False)\n",
        "        self.word_classifier.weight = shared_weight\n",
        "\n",
        "    def forward(self, tokens, segments, masked_pos):\n",
        "        output = self.embedding(tokens, segments)\n",
        "        enc_self_pad_mask = get_pad_mask(tokens)\n",
        "        for layer in self.encoders:\n",
        "            output = layer(output, enc_self_pad_mask)\n",
        "        # output: [batch, max_len, d_model]\n",
        "\n",
        "        # NSP Task\n",
        "        hidden_pool = self.pooler(output[:, 0])\n",
        "        logits_cls = self.next_cls(hidden_pool)\n",
        "\n",
        "        # Masked Language Model Task\n",
        "        # masked_pos: [batch, max_pred] -> [batch, max_pred, d_model]\n",
        "        masked_pos = masked_pos.unsqueeze(-1).expand(-1, -1, d_model)\n",
        "\n",
        "        # h_masked: [batch, max_pred, d_model]\n",
        "        h_masked = torch.gather(output, dim=1, index=masked_pos)\n",
        "        h_masked = self.gelu(self.fc(h_masked))\n",
        "        logits_lm = self.word_classifier(h_masked)\n",
        "        # logits_lm: [batch, max_pred, max_vocab]\n",
        "        # logits_cls: [batch, 2]\n",
        "\n",
        "        return logits_cls, logits_lm"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Xt2qLaxFc1s"
      },
      "source": [
        "test_text = (\n",
        "    'Hello, how are you? I am Romeo.\\n'  # R\n",
        "    'Hello, Romeo My name is Juliet. Nice to meet you.\\n'  # J\n",
        "    'Nice meet you too. How are you today?\\n'  # R\n",
        "    'Great. My baseball team won the competition.\\n'  # J\n",
        "    'Oh Congratulations, Juliet\\n'  # R\n",
        "    'Thank you Romeo\\n'  # J\n",
        "    'Where are you going today?\\n'  # R\n",
        "    'I am going shopping. What about you?\\n'  # J\n",
        "    'I am going to visit my grandmother. she is not very well'  # R\n",
        ")\n",
        "\n",
        "# we need [MASK] [SEP] [PAD] [CLS]\n",
        "word2idx = {f'[{name}]': idx for idx,\n",
        "            name in enumerate(['PAD', 'CLS', 'SEP', 'MASK'])}\n",
        "# {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
        "\n",
        "sentences = re.sub(\"[.,!?\\\\-]\", '', test_text.lower()).split('\\n')\n",
        "word_list = list(set(\" \".join(sentences).split()))\n",
        "\n",
        "holdplace = len(word2idx)\n",
        "for idx, word in enumerate(word_list):\n",
        "    word2idx[word] = idx + holdplace\n",
        "\n",
        "idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "vocab_size = len(word2idx)\n",
        "assert len(word2idx) == len(idx2word)\n",
        "\n",
        "token_list = []\n",
        "for sentence in sentences:\n",
        "    token_list.append([\n",
        "        word2idx[s] for s in sentence.split()\n",
        "    ])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBokRvZ8FQWz"
      },
      "source": [
        "def padding(ids, n_pads, pad_symb=0):\n",
        "    return ids.extend([pad_symb for _ in range(n_pads)])\n",
        "\n",
        "\n",
        "def masking_procedure(cand_pos, input_ids, masked_symb='[MASK]'):\n",
        "    masked_pos = []\n",
        "    masked_tokens = []\n",
        "    for pos in cand_pos:\n",
        "        masked_pos.append(pos)\n",
        "        masked_tokens.append(input_ids[pos])\n",
        "        if random.random() < p_mask:\n",
        "            input_ids[pos] = masked_symb\n",
        "        elif random.random() > (p_mask + p_replace):\n",
        "            rand_word_idx = random.randint(4, vocab_size - 1)\n",
        "            input_ids[pos] = rand_word_idx\n",
        "\n",
        "    return masked_pos, masked_tokens"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ot09iMRMFiRq"
      },
      "source": [
        "def make_data(sentences, n_data):\n",
        "    batch_data = []\n",
        "    positive = negative = 0\n",
        "    len_sentences = len(sentences)\n",
        "    # 50% sampling adjacent sentences, 50% sampling not adjacent sentences\n",
        "    while positive != n_data / 2 or negative != n_data / 2:\n",
        "        tokens_a_idx = random.randrange(len_sentences)\n",
        "        tokens_b_idx = random.randrange(len_sentences)\n",
        "        tokens_a = sentences[tokens_a_idx]\n",
        "        tokens_b = sentences[tokens_b_idx]\n",
        "\n",
        "        input_ids = [word2idx['[CLS]']] + tokens_a + [word2idx['[SEP]']] + tokens_b + [word2idx['[SEP]']]\n",
        "        segment_ids = [0 for i in range(\n",
        "            1 + len(tokens_a) + 1)] + [1 for i in range(1 + len(tokens_b))]\n",
        "\n",
        "        n_pred = min(max_pred, max(1, int(len(input_ids) * .15)))\n",
        "        cand_pos = [i for i, token in enumerate(input_ids)\n",
        "                    if token != word2idx['[CLS]'] and token != word2idx['[SEP]']]\n",
        "        random.shuffle(cand_pos)\n",
        "        # shuffle all candidate position index, to sampling maksed position from first n_pred\n",
        "        masked_pos, masked_tokens = masking_procedure(\n",
        "            cand_pos[:n_pred], input_ids, word2idx['[MASK]'])\n",
        "\n",
        "        # zero padding for tokens\n",
        "        padding(input_ids, max_len - len(input_ids))\n",
        "        padding(segment_ids, max_len - len(segment_ids))\n",
        "\n",
        "        # zero padding for mask\n",
        "        if max_pred > n_pred:\n",
        "            n_pads = max_pred - n_pred\n",
        "            padding(masked_pos, n_pads)\n",
        "            padding(masked_tokens, n_pads)\n",
        "\n",
        "        if (tokens_a_idx + 1) == tokens_b_idx and positive < (n_data / 2):\n",
        "            batch_data.append(\n",
        "                [input_ids, segment_ids, masked_tokens, masked_pos, True])\n",
        "            positive += 1\n",
        "        elif (tokens_a_idx + 1) != tokens_b_idx and negative < (n_data / 2):\n",
        "            batch_data.append(\n",
        "                [input_ids, segment_ids, masked_tokens, masked_pos, False])\n",
        "            negative += 1\n",
        "\n",
        "    return batch_data\n",
        "\n",
        "\n",
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, input_ids, segment_ids, masked_tokens, masked_pos, is_next):\n",
        "        super(BERTDataset, self).__init__()\n",
        "        self.input_ids = input_ids\n",
        "        self.segment_ids = segment_ids\n",
        "        self.masked_tokens = masked_tokens\n",
        "        self.masked_pos = masked_pos\n",
        "        self.is_next = is_next\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.input_ids[index], self.segment_ids[index], self.masked_tokens[index], self.masked_pos[index], self.is_next[index]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5xmxDBzFXGi",
        "outputId": "5990ee4a-3c52-4412-ec9b-7b229730d563"
      },
      "source": [
        "batch_size = 6\n",
        "batch_data = make_data(token_list, n_data=batch_size)\n",
        "batch_tensor = [torch.LongTensor(ele) for ele in zip(*batch_data)]\n",
        "\n",
        "dataset = BERTDataset(*batch_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model = BERT(n_layers)\n",
        "lr = 1e-3\n",
        "epochs = 300\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adadelta(model.parameters(), lr=lr)\n",
        "model.to(device)\n",
        "\n",
        "# training\n",
        "for epoch in range(epochs):\n",
        "    for one_batch in dataloader:\n",
        "        input_ids, segment_ids, masked_tokens, masked_pos, is_next = [ele.to(device) for ele in one_batch]\n",
        "\n",
        "        logits_cls, logits_lm = model(input_ids, segment_ids, masked_pos)\n",
        "        loss_cls = criterion(logits_cls, is_next)\n",
        "        loss_lm = criterion(logits_lm.view(-1, max_vocab), masked_tokens.view(-1))\n",
        "        loss_lm = (loss_lm.float()).mean()\n",
        "        loss = loss_cls + loss_lm\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch:{epoch + 1} \\t loss: {loss:.6f}')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:10 \t loss: 4.968745\n",
            "Epoch:20 \t loss: 2.511611\n",
            "Epoch:30 \t loss: 1.357405\n",
            "Epoch:40 \t loss: 0.838513\n",
            "Epoch:50 \t loss: 0.792048\n",
            "Epoch:60 \t loss: 0.842023\n",
            "Epoch:70 \t loss: 0.730047\n",
            "Epoch:80 \t loss: 0.771186\n",
            "Epoch:90 \t loss: 0.698691\n",
            "Epoch:100 \t loss: 0.674331\n",
            "Epoch:110 \t loss: 0.681281\n",
            "Epoch:120 \t loss: 0.668732\n",
            "Epoch:130 \t loss: 0.663461\n",
            "Epoch:140 \t loss: 0.731162\n",
            "Epoch:150 \t loss: 0.756719\n",
            "Epoch:160 \t loss: 0.678658\n",
            "Epoch:170 \t loss: 0.705775\n",
            "Epoch:180 \t loss: 0.627448\n",
            "Epoch:190 \t loss: 0.625969\n",
            "Epoch:200 \t loss: 0.660304\n",
            "Epoch:210 \t loss: 0.669675\n",
            "Epoch:220 \t loss: 0.641547\n",
            "Epoch:230 \t loss: 0.655259\n",
            "Epoch:240 \t loss: 0.648478\n",
            "Epoch:250 \t loss: 0.648678\n",
            "Epoch:260 \t loss: 0.566995\n",
            "Epoch:270 \t loss: 0.570572\n",
            "Epoch:280 \t loss: 0.614088\n",
            "Epoch:290 \t loss: 0.721956\n",
            "Epoch:300 \t loss: 0.609441\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JzkOvFm6AY2",
        "outputId": "24eb01ae-6a66-4688-c736-59626fbbee5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Using one sentence to test\n",
        "test_data_idx = 3\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    input_ids, segment_ids, masked_tokens, masked_pos, is_next = batch_data[test_data_idx]\n",
        "    input_ids = torch.LongTensor(input_ids).unsqueeze(0).to(device)\n",
        "    segment_ids = torch.LongTensor(segment_ids).unsqueeze(0).to(device)\n",
        "    masked_pos = torch.LongTensor(masked_pos).unsqueeze(0).to(device)\n",
        "    masked_tokens = torch.LongTensor(masked_tokens).unsqueeze(0).to(device)\n",
        "    logits_cls, logits_lm = model(input_ids, segment_ids, masked_pos)\n",
        "    input_ids, segment_ids, masked_tokens, masked_pos, is_next = batch_data[test_data_idx]\n",
        "    print(\"========================================================\")\n",
        "    print(\"Masked data:\")\n",
        "    masked_sentence = [idx2word[w] for w in input_ids if idx2word[w] != '[PAD]']\n",
        "    print(masked_sentence)\n",
        "\n",
        "    # logits_lm: [batch, max_pred, max_vocab]\n",
        "    # logits_cls: [batch, 2]\n",
        "    cpu = torch.device('cpu')\n",
        "    pred_mask = logits_lm.data.max(2)[1][0].to(cpu).numpy()\n",
        "    pred_next = logits_cls.data.max(1)[1].data.to(cpu).numpy()[0]\n",
        "\n",
        "    bert_sentence = masked_sentence.copy()\n",
        "    original_sentence = masked_sentence.copy()\n",
        "\n",
        "    for i in range(len(masked_pos)):\n",
        "        pos = masked_pos[i]\n",
        "        if pos == 0:\n",
        "            break\n",
        "        bert_sentence[pos] = idx2word[pred_mask[i]]\n",
        "        original_sentence[pos] = idx2word[masked_tokens[i]]\n",
        "\n",
        "    print(\"BERT reconstructed:\")\n",
        "    print(bert_sentence)\n",
        "    print(\"Original sentence:\")\n",
        "    print(original_sentence)\n",
        "\n",
        "    print(\"===============Next Sentence Prediction===============\")\n",
        "    print(f'Two sentences are continuous? {True if is_next else False}')\n",
        "    print(f'BERT predict: {True if pred_next else False}')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================================\n",
            "Masked data:\n",
            "['[CLS]', 'hello', 'how', 'are', 'you', 'i', 'am', 'romeo', '[SEP]', 'hello', '[MASK]', 'my', 'name', 'very', 'juliet', 'nice', '[MASK]', 'meet', 'you', '[SEP]']\n",
            "BERT reconstructed:\n",
            "['[CLS]', 'hello', 'how', 'are', 'you', 'i', 'am', 'romeo', '[SEP]', 'hello', 'romeo', 'my', 'name', 'is', 'juliet', 'nice', 'to', 'meet', 'you', '[SEP]']\n",
            "Original sentence:\n",
            "['[CLS]', 'hello', 'how', 'are', 'you', 'i', 'am', 'romeo', '[SEP]', 'hello', 'romeo', 'my', 'name', 'is', 'juliet', 'nice', 'to', 'meet', 'you', '[SEP]']\n",
            "===============Next Sentence Prediction===============\n",
            "Two sentences are continuous? True\n",
            "BERT predict: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnASyjCg3RHs"
      },
      "source": [],
      "execution_count": 15,
      "outputs": []
    }
  ]
}